{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925f544f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load all datasets\n",
    "file_path = \"../data/repositoryinfo/peer_auth_v1_repos.parquet\"\n",
    "df_peer_auth_v1 = pd.read_parquet(file_path).assign(kind=\"peer\", version=\"v1\")\n",
    "\n",
    "file_path = \"../data/repositoryinfo/req_auth_v1_repos.parquet\"\n",
    "df_req_auth_v1 = pd.read_parquet(file_path).assign(kind=\"request\", version=\"v1\")\n",
    "\n",
    "file_path = \"../data/repositoryinfo/peer_auth_v1beta1_repos.parquet\"\n",
    "df_peer_auth_v1beta1 = pd.read_parquet(file_path).assign(kind=\"peer\", version=\"v1beta1\")\n",
    "\n",
    "file_path = \"../data/repositoryinfo/req_auth_v1beta1_repos.parquet\"\n",
    "df_req_auth_v1beta1 = pd.read_parquet(file_path).assign(kind=\"request\", version=\"v1beta1\")\n",
    "\n",
    "file_path = \"../data/repositoryinfo/peer_auth_v1alpha1_repos.parquet\"\n",
    "df_peer_auth_v1alpha1 = pd.read_parquet(file_path).assign(kind=\"peer\", version=\"v1alpha1\")\n",
    "\n",
    "file_path = \"../data/repositoryinfo/req_auth_v1alpha1_repos.parquet\"\n",
    "df_req_auth_v1alpha1 = pd.read_parquet(file_path).assign(kind=\"request\", version=\"v1alpha1\")\n",
    "\n",
    "# Combine everything\n",
    "df_all_auth = pd.concat([\n",
    "    df_peer_auth_v1,\n",
    "    df_req_auth_v1,\n",
    "    df_peer_auth_v1beta1,\n",
    "    df_req_auth_v1beta1,\n",
    "    df_peer_auth_v1alpha1,\n",
    "    df_req_auth_v1alpha1\n",
    "], ignore_index=True)\n",
    "\n",
    "# Assuming each dataframe has a 'repo' column (adjust if it's named differently)\n",
    "repo_sets = {\n",
    "    \"peer_v1\": set(df_peer_auth_v1[\"full_name\"]),\n",
    "    \"req_v1\": set(df_req_auth_v1[\"full_name\"]),\n",
    "    \"peer_v1beta1\": set(df_peer_auth_v1beta1[\"full_name\"]),\n",
    "    \"req_v1beta1\": set(df_req_auth_v1beta1[\"full_name\"]),\n",
    "    \"peer_v1alpha1\": set(df_peer_auth_v1alpha1[\"full_name\"]),\n",
    "    \"req_v1alpha1\": set(df_req_auth_v1alpha1[\"full_name\"]),\n",
    "}\n",
    "\n",
    "# Calculate disjoint sets (repos unique to each group)\n",
    "disjoint_sets = {\n",
    "    key: repos - set.union(*(v for k,v in repo_sets.items() if k != key))\n",
    "    for key, repos in repo_sets.items()\n",
    "}\n",
    "\n",
    "# Create summary dataframe\n",
    "summary_df = pd.DataFrame({\n",
    "    \"group\": list(disjoint_sets.keys()),\n",
    "    \"unique_repos_count\": [len(s) for s in disjoint_sets.values()]\n",
    "})\n",
    "\n",
    "\n",
    "summary_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b920cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.font_manager as font_manager\n",
    "\n",
    "def saveFigure(fig,fileName):\n",
    "    fig.savefig(fileName,bbox_inches='tight',dpi=100, pad_inches = 0)\n",
    "\n",
    "\n",
    "def get_font_properties(routePath=os.getcwd()):\n",
    "    \n",
    "    fontpath = routePath+'/NimbusSanL-Reg.otf'\n",
    "    prop = font_manager.FontProperties(fname=fontpath, size=16)\n",
    "    return prop\n",
    "\n",
    "def get_props_as_dict():\n",
    "    routePath=os.getcwd()\n",
    "    fontpath = routePath+'/NimbusSanL-Reg.otf'\n",
    "    return {\n",
    "        \"name\": fontpath,\n",
    "        \"size\": 26\n",
    "    }\n",
    "\n",
    "def get_alt_font_properties(type=\"san\",size=15):\n",
    "    routePath=os.getcwd()\n",
    "    if type == \"rom\":\n",
    "        fontpath = routePath+'/NimbusRomNo9L-Reg.otf'\n",
    "    else:\n",
    "        fontpath = routePath+'/NimbusSanL-Reg.otf'\n",
    "    prop = font_manager.FontProperties(fname=fontpath, size=size)\n",
    "    return prop\n",
    "\n",
    "def save_fig(ax, title):\n",
    "    ax.figure.savefig(title,  dpi=300,  bbox_inches='tight', format=\"pdf\", pad_inches = 0)\n",
    "\n",
    "\n",
    "fontsize = 17\n",
    "legend_fontsize = fontsize\n",
    "smallfontsize = fontsize -1\n",
    "\n",
    "prop = get_alt_font_properties(type=\"rom\",size=fontsize)\n",
    "prop_small = get_alt_font_properties(type=\"rom\",size=smallfontsize)\n",
    "prop_legend = get_alt_font_properties(type=\"rom\",size=legend_fontsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f8a139",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([df_peer_auth_v1,df_req_auth_v1,df_peer_auth_v1beta1,df_req_auth_v1beta1,df_peer_auth_v1alpha1,df_req_auth_v1alpha1 ], ignore_index=True)\n",
    "df_req=pd.concat([df_req_auth_v1,df_req_auth_v1beta1,df_req_auth_v1alpha1 ], ignore_index=True)\n",
    "df_peer=pd.concat([df_peer_auth_v1,df_peer_auth_v1beta1,df_peer_auth_v1alpha1 ], ignore_index=True)\n",
    "df_v1=pd.concat([df_peer_auth_v1,df_req_auth_v1 ], ignore_index=True)\n",
    "df_v1beta1=pd.concat([df_peer_auth_v1beta1,df_req_auth_v1beta1 ], ignore_index=True)\n",
    "df_v1alpha1=pd.concat([df_peer_auth_v1alpha1,df_req_auth_v1alpha1 ], ignore_index=True)\n",
    "\n",
    "df_all_unique = df_all.drop_duplicates(subset=\"full_name\")\n",
    "df_req_auth_unique = df_req.drop_duplicates(subset=\"full_name\")\n",
    "df_peer_auth_unique = df_peer.drop_duplicates(subset=\"full_name\")\n",
    "df_v1_unique = df_v1.drop_duplicates(subset=\"full_name\")\n",
    "df_v1beta1_unique = df_v1beta1.drop_duplicates(subset=\"full_name\")\n",
    "df_v1alpha1_unique = df_v1alpha1.drop_duplicates(subset=\"full_name\")\n",
    "print(\"All:\", len(df_all_unique))\n",
    "print(\"PeerAuthentication:\", len(df_peer_auth_unique))\n",
    "print(\"RequestAuthentication:\", len(df_req_auth_unique))\n",
    "print(\"v1:\", len(df_v1_unique)-len(df_v1beta1_unique))\n",
    "print(\"v1beta1:\", len(df_v1beta1_unique))\n",
    "print(\"v1alpha1:\", len(df_v1alpha1_unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51282f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_all_star = df_all_unique[df_all_unique[\"stargazers_count\"] >= 10]\n",
    "df_req_auth_star = df_req_auth_unique[df_req_auth_unique[\"stargazers_count\"] >= 10]\n",
    "df_peer_auth_star = df_peer_auth_unique[df_peer_auth_unique[\"stargazers_count\"] >= 10]\n",
    "df_v1_star = df_v1_unique[df_v1_unique[\"stargazers_count\"] >= 10]\n",
    "df_v1beta1_star = df_v1beta1_unique[df_v1beta1_unique[\"stargazers_count\"] >= 10]\n",
    "df_v1alpha1_star = df_v1alpha1_unique[df_v1alpha1_unique[\"stargazers_count\"] >= 10]\n",
    "\n",
    "print(\"All:\", len(df_all_star))\n",
    "print(\"PeerAuthentication:\", len(df_peer_auth_star))\n",
    "print(\"RequestAuthentication:\", len(df_req_auth_star))\n",
    "print(\"v1:\", len(df_v1_star) - len(df_v1beta1_star))\n",
    "print(\"v1beta1:\", len(df_v1beta1_star))\n",
    "print(\"v1alpha1:\", len(df_v1alpha1_star))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3bf8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Build summary counts for unique vs star repos\n",
    "counts = {\n",
    "    \"all\": (len(df_all_unique), len(df_all_star)),\n",
    "    \"req_auth\": (len(df_req_auth_unique), len(df_req_auth_star)),\n",
    "    \"peer_auth\": (len(df_peer_auth_unique), len(df_peer_auth_star)),\n",
    "    \"v1\": (len(df_v1_unique), len(df_v1_star)),\n",
    "    \"v1beta1\": (len(df_v1beta1_unique), len(df_v1beta1_star)),\n",
    "    \"v1alpha1\": (len(df_v1alpha1_unique), len(df_v1alpha1_star)),\n",
    "}\n",
    "\n",
    "# Convert to dataframe for plotting\n",
    "plot_df = pd.DataFrame(\n",
    "    [(k, \"unique\", v[0]) for k, v in counts.items()] +\n",
    "    [(k, \"star\", v[1]) for k, v in counts.items()],\n",
    "    columns=[\"group\", \"type\", \"count\"]\n",
    ")\n",
    "\n",
    "# Plot grouped bar chart\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "plot_df.pivot(index=\"group\", columns=\"type\", values=\"count\").plot(\n",
    "    kind=\"bar\", ax=ax, rot=0\n",
    ")\n",
    "\n",
    "ax.set_title(\"Unique vs Star (>=10 stars) Repositories per Category\")\n",
    "ax.set_ylabel(\"full_namesitory Count\")\n",
    "ax.set_xlabel(\"Category\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80f190e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Individual version sets\n",
    "set_v1 = set(df_v1_unique[\"full_name\"])\n",
    "set_v1beta1 = set(df_v1beta1_unique[\"full_name\"])\n",
    "set_v1alpha1 = set(df_v1alpha1_unique[\"full_name\"])\n",
    "\n",
    "# Compute intersections\n",
    "counts = {\n",
    "    \"v1\": len(set_v1),\n",
    "    \"v1beta1\": len(set_v1beta1),\n",
    "    \"v1alpha1\": len(set_v1alpha1),\n",
    "    \"v1 ∩ v1beta1\": len(set_v1 & set_v1beta1),\n",
    "    \"v1 ∩ v1alpha1\": len(set_v1 & set_v1alpha1),\n",
    "    \"v1beta1 ∩ v1alpha1\": len(set_v1beta1 & set_v1alpha1),\n",
    "    \"v1 ∩ v1beta1 ∩ v1alpha1\": len(set_v1 & set_v1beta1 & set_v1alpha1)\n",
    "}\n",
    "\n",
    "# Convert to dataframe for plotting\n",
    "plot_df = pd.DataFrame({\n",
    "    \"group\": list(counts.keys()),\n",
    "    \"count\": list(counts.values())\n",
    "})\n",
    "\n",
    "# Plot horizontal bar chart\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "bars = ax.barh(plot_df[\"group\"], plot_df[\"count\"], color=\"#1f77b4\")\n",
    "\n",
    "# Annotate counts\n",
    "for bar in bars:\n",
    "    width = bar.get_width()\n",
    "    y = bar.get_y() + bar.get_height()/2\n",
    "    ax.text(width + 2, y, str(width), ha='left', va='center')\n",
    "\n",
    "ax.set_xlabel(\"Number of Unique Repositories\")\n",
    "ax.set_title(\"Unique Repositories per Version and Overlaps\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de647ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_venn import venn3\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "# Precomputed counts\n",
    "only_v1 = len(set_v1 - set_v1beta1 - set_v1alpha1)\n",
    "only_v1beta1 = len(set_v1beta1 - set_v1 - set_v1alpha1)\n",
    "only_v1alpha1 = len(set_v1alpha1 - set_v1 - set_v1beta1)\n",
    "\n",
    "v1_and_v1beta1 = len((set_v1 & set_v1beta1) - set_v1alpha1)\n",
    "v1_and_v1alpha1 = len((set_v1 & set_v1alpha1) - set_v1beta1)\n",
    "v1beta1_and_v1alpha1 = len((set_v1beta1 & set_v1alpha1) - set_v1)\n",
    "all_three = len(set_v1 & set_v1beta1 & set_v1alpha1)\n",
    "\n",
    "# Colors\n",
    "colors = ('#1f77b4', '#ff7f0e', '#2ca02c')\n",
    "\n",
    "# Create Venn diagram\n",
    "plt.figure(figsize=(8,8))\n",
    "v = venn3(subsets=(only_v1, only_v1beta1, v1_and_v1beta1,\n",
    "                   only_v1alpha1, v1_and_v1alpha1,\n",
    "                   v1beta1_and_v1alpha1, all_three),\n",
    "          set_labels=('', '', ''),\n",
    "          set_colors=colors,\n",
    "          alpha=0.5)\n",
    "\n",
    "# Legend\n",
    "legend_elements = [Patch(facecolor=colors[0], alpha=0.5, label='v1'),\n",
    "                   Patch(facecolor=colors[1], alpha=0.5, label='beta'),\n",
    "                   Patch(facecolor=colors[2], alpha=0.5, label='alpha')]\n",
    "plt.legend(handles=legend_elements, loc='upper right', prop=prop)\n",
    "\n",
    "# Add small labels for set combinations\n",
    "subset_labels = {\n",
    "    '100': 'v1',\n",
    "    '010': 'beta',\n",
    "    '001': 'alpha',\n",
    "    '110': 'v1 ∪ beta',\n",
    "    '101': 'v1 ∪ alpha',\n",
    "    '011': 'beta ∪ alpha',\n",
    "    '111': 'v1 ∪ beta ∪ alpha'\n",
    "}\n",
    "\n",
    "for subset_id, text in subset_labels.items():\n",
    "    label = v.get_label_by_id(subset_id)\n",
    "    if label is not None:\n",
    "        # Add count + small label\n",
    "        current_text = label.get_text()\n",
    "        label.set_text(f\"{current_text}\\n{text}\")\n",
    "        # Apply font\n",
    "        label.set_fontproperties(prop)\n",
    "        # Optional: adjust position slightly\n",
    "        x, y = label.get_position()\n",
    "        label.set_position((x, y + 0.01))\n",
    "\n",
    "#plt.title(\"Repositories including Authentication Across Versions\", fontproperties=prop)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./figures/authentication_venn.svg\", dpi=300, bbox_inches=\"tight\", pad_inches=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062a2eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_venn import venn3\n",
    "from matplotlib.patches import Patch\n",
    "import numpy as np\n",
    "\n",
    "# Precomputed counts\n",
    "only_v1 = len(set_v1 - set_v1beta1 - set_v1alpha1)\n",
    "only_v1beta1 = len(set_v1beta1 - set_v1 - set_v1alpha1)\n",
    "only_v1alpha1 = len(set_v1alpha1 - set_v1 - set_v1beta1)\n",
    "\n",
    "v1_and_v1beta1 = len((set_v1 & set_v1beta1) - set_v1alpha1)\n",
    "v1_and_v1alpha1 = len((set_v1 & set_v1alpha1) - set_v1beta1)\n",
    "v1beta1_and_v1alpha1 = len((set_v1beta1 & set_v1alpha1) - set_v1)\n",
    "all_three = len(set_v1 & set_v1beta1 & set_v1alpha1)\n",
    "\n",
    "# Apply log scaling (add 1 to avoid log(0))\n",
    "def log_scale(x):\n",
    "    return np.log10(x + 1)\n",
    "\n",
    "subsets = (\n",
    "    log_scale(only_v1),\n",
    "    log_scale(only_v1beta1),\n",
    "    log_scale(v1_and_v1beta1),\n",
    "    log_scale(only_v1alpha1),\n",
    "    log_scale(v1_and_v1alpha1),\n",
    "    log_scale(v1beta1_and_v1alpha1),\n",
    "    log_scale(all_three)\n",
    ")\n",
    "\n",
    "# Colors\n",
    "colors = ('#1f77b4', '#ff7f0e', '#2ca02c')\n",
    "\n",
    "# Create Venn diagram with log-scaled sizes\n",
    "plt.figure(figsize=(8,8))\n",
    "v = venn3(subsets=subsets,\n",
    "          set_labels=('', '', ''),\n",
    "          set_colors=colors,\n",
    "          alpha=0.5)\n",
    "\n",
    "# Legend\n",
    "legend_elements = [Patch(facecolor=colors[0], alpha=0.5, label='v1'),\n",
    "                   Patch(facecolor=colors[1], alpha=0.5, label='beta'),\n",
    "                   Patch(facecolor=colors[2], alpha=0.5, label='alpha')]\n",
    "plt.legend(handles=legend_elements, loc='upper right', prop=prop)\n",
    "\n",
    "# Add small labels for set combinations\n",
    "subset_labels = {\n",
    "    '100': 'v1',\n",
    "    '010': 'beta',\n",
    "    '001': 'alpha',\n",
    "    '110': 'v1 ∪ beta',\n",
    "    '101': 'v1 ∪ alpha',\n",
    "    '011': 'beta ∪ alpha',\n",
    "    '111': 'v1 ∪ beta ∪ alpha'\n",
    "}\n",
    "\n",
    "for subset_id, text in subset_labels.items():\n",
    "    label = v.get_label_by_id(subset_id)\n",
    "    if label is not None:\n",
    "        # Original raw count\n",
    "        if subset_id == '100':\n",
    "            count = only_v1\n",
    "        elif subset_id == '010':\n",
    "            count = only_v1beta1\n",
    "        elif subset_id == '001':\n",
    "            count = only_v1alpha1\n",
    "        elif subset_id == '110':\n",
    "            count = v1_and_v1beta1\n",
    "        elif subset_id == '101':\n",
    "            count = v1_and_v1alpha1\n",
    "        elif subset_id == '011':\n",
    "            count = v1beta1_and_v1alpha1\n",
    "        elif subset_id == '111':\n",
    "            count = all_three\n",
    "\n",
    "        # Add count + small label\n",
    "        label.set_text(f\"{count}\\n{text}\")\n",
    "        label.set_fontproperties(prop)\n",
    "        # Optional: adjust position slightly\n",
    "        x, y = label.get_position()\n",
    "        label.set_position((x, y + 0.01))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./figures/authentication_venn.svg\", dpi=300, bbox_inches=\"tight\", pad_inches=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343ceb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_venn import venn3\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "# Precomputed counts\n",
    "only_v1 = len(set_v1 - set_v1beta1 - set_v1alpha1)\n",
    "only_v1beta1 = len(set_v1beta1 - set_v1 - set_v1alpha1)\n",
    "only_v1alpha1 = len(set_v1alpha1 - set_v1 - set_v1beta1)\n",
    "\n",
    "v1_and_v1beta1 = len((set_v1 & set_v1beta1) - set_v1alpha1)\n",
    "v1_and_v1alpha1 = len((set_v1 & set_v1alpha1) - set_v1beta1)\n",
    "v1beta1_and_v1alpha1 = len((set_v1beta1 & set_v1alpha1) - set_v1)\n",
    "all_three = len(set_v1 & set_v1beta1 & set_v1alpha1)\n",
    "\n",
    "# Colors\n",
    "colors = ('#1f77b4', '#ff7f0e', '#2ca02c')\n",
    "\n",
    "# Fixed circle sizes\n",
    "fixed_sizes = (1, 1, 1, 1, 1, 1, 1)\n",
    "\n",
    "# Reduce figure size but keep font size\n",
    "plt.figure(figsize=(5,5))  # smaller than 8x8\n",
    "v = venn3(subsets=fixed_sizes,\n",
    "          set_labels=('', '', ''),\n",
    "          set_colors=colors,\n",
    "          alpha=0.5)\n",
    "\n",
    "# Legend\n",
    "legend_elements = [Patch(facecolor=colors[0], alpha=0.5, label='v1'),\n",
    "                   Patch(facecolor=colors[1], alpha=0.5, label='beta'),\n",
    "                   Patch(facecolor=colors[2], alpha=0.5, label='alpha')]\n",
    "plt.legend(\n",
    "    handles=legend_elements,\n",
    "    loc='upper right',\n",
    "    prop=prop,\n",
    "    bbox_to_anchor=(1, 1.05)  # x, y relative to axes; <1 moves it inward\n",
    ")\n",
    "\n",
    "# Add labels with counts and union info\n",
    "subset_counts = {\n",
    "    '100': only_v1,\n",
    "    '010': only_v1beta1,\n",
    "    '001': only_v1alpha1,\n",
    "    '110': v1_and_v1beta1,\n",
    "    '101': v1_and_v1alpha1,\n",
    "    '011': v1beta1_and_v1alpha1,\n",
    "    '111': all_three\n",
    "}\n",
    "\n",
    "subset_labels = {\n",
    "    '100': 'v1',\n",
    "    '010': 'beta',\n",
    "    '001': 'alpha',\n",
    "    '110': 'v1 ∪ beta',\n",
    "    '101': 'v1 ∪ alpha',\n",
    "    '011': 'beta ∪ alpha',\n",
    "    '111': 'v1 ∪ beta ∪ alpha'\n",
    "}\n",
    "\n",
    "for subset_id, text in subset_labels.items():\n",
    "    label = v.get_label_by_id(subset_id)\n",
    "    if label is not None:\n",
    "        count = subset_counts[subset_id]\n",
    "        label.set_text(f\"{count}\\n{text}\")\n",
    "        label.set_fontproperties(prop)\n",
    "        x, y = label.get_position()\n",
    "        label.set_position((x, y + 0.01))  # adjust slightly upward\n",
    "\n",
    "plt.tight_layout(pad=0)\n",
    "plt.savefig(\"./figures/authentication_venn_fixed_small.svg\", dpi=300, bbox_inches=\"tight\", pad_inches=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dc7cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you do not need to run, this code is for presetitnh \n",
    "import dask.dataframe as dd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "files_v1 = [\n",
    "    \"../data/commitinfo/provider_authz_v1_commit.parquet\",\n",
    "    \"../data/commitinfo/req_auth_v1_commit.parquet\",\n",
    "    \"../data/commitinfo/http_authz_v1_commit.parquet\",\n",
    "    \"../data/commitinfo/tcp_authz_v1_commit.parquet\",\n",
    "    \"../data/commitinfo/jwt_authz_v1_commit.parquet\",\n",
    "    \"../data/commitinfo/provider_authz_v1_commit.parquet\",\n",
    "    \"../data/commitinfo/ingress_authz_ip_v1_commit.parquet\",\n",
    "    \"../data/commitinfo/ingress_authz_remote_ip_v1_commit.parquet\",\n",
    "    \"../data/commitinfo/mtls_strict_v1_commit.parquet\",\n",
    "    \"../data/commitinfo/mtls_permissive_v1_commit.parquet\",\n",
    "    \"../data/commitinfo/mtls_disable_v1_commit.parquet\",\n",
    "]\n",
    "\n",
    "files_v1beta1 = [\n",
    "    \"../data/commitinfo/peer_auth_v1beta1_commit.parquet\",\n",
    "    \"../data/commitinfo/req_auth_v1beta1_commit.parquet\",\n",
    "    \"../data/commitinfo/http_authz_v1beta1_commit.parquet\",\n",
    "    \"../data/commitinfo/tcp_authz_v1beta1_commit.parquet\",\n",
    "    \"../data/commitinfo/jwt_authz_v1beta1_commit.parquet\",\n",
    "    \"../data/commitinfo/provider_authz_v1beta1_commit.parquet\",\n",
    "    \"../data/commitinfo/ingress_authz_ip_v1beta1_commit.parquet\",\n",
    "    \"../data/commitinfo/ingress_authz_remote_ip_v1beta1_commit.parquet\",\n",
    "    \"../data/commitinfo/mtls_strict_v1beta1_commit.parquet\",\n",
    "    \"../data/commitinfo/mtls_permissive_v1beta1_commit.parquet\",\n",
    "    \"../data/commitinfo/mtls_disable_v1beta1_commit.parquet\",\n",
    "]\n",
    "\n",
    "files_v1alpha1 = [\n",
    "    \"../data/commitinfo/peer_auth_v1alpha1_commit.parquet\",\n",
    "    \"../data/commitinfo/req_auth_v1alpha1_commit.parquet\",\n",
    "    \"../data/commitinfo/any_authz_v1alpha1_commit.parquet\",\n",
    "    \"../data/commitinfo/mtls_strict_v1alpha1_commit.parquet\",\n",
    "    \"../data/commitinfo/mtls_permissive_v1alpha1_commit.parquet\",\n",
    "]\n",
    "\n",
    "\n",
    "processed_dirs = {\n",
    "    \"v1\": \"../data/processed/v1\",\n",
    "    \"v1beta1\": \"../data/processed/v1beta1\",\n",
    "    \"v1alpha1\": \"../data/processed/v1alpha1\",\n",
    "}\n",
    "\n",
    "final_dirs = {\n",
    "    \"v1\": \"../data/final/v1\",\n",
    "    \"v1beta1\": \"../data/final/v1beta1\",\n",
    "    \"v1alpha1\": \"../data/final/v1alpha1\",\n",
    "}\n",
    "\n",
    "def extract_and_save(file_list, out_dir):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    for f in file_list:\n",
    "        print(f\"Processing {f}...\")\n",
    "        try:\n",
    "            ddf = dd.read_parquet(f, columns=['sha', 'commit', 'repository_full_name'], blocksize=\"128MB\")\n",
    "\n",
    "\n",
    "            def extract(df):\n",
    "                df['author_date'] = df['commit'].apply(lambda x: x['author']['date'])\n",
    "                return df[['sha', 'author_date', 'repository_full_name']]\n",
    "\n",
    "            ddf = ddf.map_partitions(\n",
    "                extract,\n",
    "                meta={'sha': 'object', 'author_date': 'object', 'repository_full_name': 'object'}\n",
    "            )\n",
    "\n",
    "  \n",
    "            ddf = ddf.map_partitions(lambda df: df.drop_duplicates(subset='sha'), meta=ddf._meta)\n",
    "\n",
    "\n",
    "            name = os.path.basename(f).replace(\".parquet\", \"\")\n",
    "            out_path = os.path.join(out_dir, f\"{name}_processed\")\n",
    "            ddf.to_parquet(out_path, overwrite=True)\n",
    "\n",
    " \n",
    "            ddf_check = dd.read_parquet(out_path)\n",
    "            if 'sha' not in ddf_check.columns:\n",
    "                print(f\"[MISSING] 'sha' column missing in saved file: {out_path}\")\n",
    "            else:\n",
    "                print(f\"[OK] 'sha' column exists in saved file: {out_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to process {f}: {e}\")\n",
    "\n",
    "def combine_and_save(dir_path, final_out):\n",
    "    files = glob.glob(os.path.join(dir_path, \"*_processed\"))\n",
    "    if not files:\n",
    "        print(f\"[WARNING] No processed files found in {dir_path}\")\n",
    "        return\n",
    "\n",
    "    all_parquet_files = []\n",
    "    for d in files:\n",
    "\n",
    "        parquet_files = glob.glob(os.path.join(d, \"*.parquet\"))\n",
    "        if parquet_files:\n",
    "            all_parquet_files.extend(parquet_files)\n",
    "        else:\n",
    "            print(f\"[SKIP] No parquet files found in {d}\")\n",
    "\n",
    "    if not all_parquet_files:\n",
    "        print(f\"[ERROR] No valid parquet files to combine in {dir_path}\")\n",
    "        return\n",
    "\n",
    "\n",
    "    ddf = dd.read_parquet(all_parquet_files, columns=['sha', 'author_date', 'repository_full_name'])\n",
    "\n",
    "\n",
    "    ddf = ddf.set_index('sha', sorted=False, drop=False)\n",
    "    ddf = ddf.map_partitions(lambda df: df[~df.index.duplicated(keep='first')])\n",
    "    ddf = ddf.reset_index(drop=True)\n",
    "\n",
    "    os.makedirs(final_out, exist_ok=True)\n",
    "    ddf.to_parquet(final_out, overwrite=True)\n",
    "    print(f\"Combined data saved to {final_out}\")\n",
    "\n",
    "# v1\n",
    "extract_and_save(files_v1, processed_dirs['v1'])\n",
    "combine_and_save(processed_dirs['v1'], final_dirs['v1'])\n",
    "\n",
    "# v1beta1\n",
    "extract_and_save(files_v1beta1, processed_dirs['v1beta1'])\n",
    "combine_and_save(processed_dirs['v1beta1'], final_dirs['v1beta1'])\n",
    "\n",
    "# v1alpha1\n",
    "extract_and_save(files_v1alpha1, processed_dirs['v1alpha1'])\n",
    "combine_and_save(processed_dirs['v1alpha1'], final_dirs['v1alpha1'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6470257",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# If set_v1alpha1 is a set, convert it to a list or Series\n",
    "alpha_repos = pd.Series(list(set_v1alpha1), name=\"full_name\")\n",
    "\n",
    "# Merge with commit info\n",
    "df_alpha_commits = pd.merge(\n",
    "    alpha_repos.to_frame(),\n",
    "    df_commits[['full_name', 'last_commit_date']],\n",
    "    on='full_name',\n",
    "    how='left'  # keep all alpha repos even if commit info is missing\n",
    ")\n",
    "\n",
    "# Optional: sort by last commit date descending\n",
    "df_alpha_commits = df_alpha_commits.sort_values(by='last_commit_date', ascending=False)\n",
    "\n",
    "df_alpha_commits.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb2c336",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib_venn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48b25d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Counts\n",
    "req_counts = [\n",
    "    len(df_v1_unique[df_v1_unique[\"kind\"] == \"request\"]),\n",
    "    len(df_v1beta1_unique[df_v1beta1_unique[\"kind\"] == \"request\"]),\n",
    "    len(df_v1alpha1_unique[df_v1alpha1_unique[\"kind\"] == \"request\"])\n",
    "]\n",
    "\n",
    "peer_counts = [\n",
    "    len(df_v1_unique[df_v1_unique[\"kind\"] == \"peer\"]),\n",
    "    len(df_v1beta1_unique[df_v1beta1_unique[\"kind\"] == \"peer\"]),\n",
    "    len(df_v1alpha1_unique[df_v1alpha1_unique[\"kind\"] == \"peer\"])\n",
    "]\n",
    "\n",
    "versions = [\"v1\", \"v1beta1\", \"v1alpha1\"]\n",
    "x = np.arange(len(versions))\n",
    "bar_width = 0.35\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "bars_req = ax.bar(x - bar_width/2, req_counts, bar_width, label='req_auth')\n",
    "bars_peer = ax.bar(x + bar_width/2, peer_counts, bar_width, label='peer_auth')\n",
    "\n",
    "# Annotate percentages INSIDE each bar (relative to version total)\n",
    "for i, (bar_req, bar_peer) in enumerate(zip(bars_req, bars_peer)):\n",
    "    total = req_counts[i] + peer_counts[i]\n",
    "    if total > 0:\n",
    "        pct_req = req_counts[i] / total * 100\n",
    "        pct_peer = peer_counts[i] / total * 100\n",
    "\n",
    "        # Put text inside each bar\n",
    "        ax.text(bar_req.get_x() + bar_req.get_width()/2, bar_req.get_height()/2,\n",
    "                f\"{pct_req:.1f}%\", ha=\"center\", va=\"center\", color=\"white\", fontsize=10, fontweight=\"bold\")\n",
    "        ax.text(bar_peer.get_x() + bar_peer.get_width()/2, bar_peer.get_height()/2,\n",
    "                f\"{pct_peer:.1f}%\", ha=\"center\", va=\"center\", color=\"white\", fontsize=10, fontweight=\"bold\")\n",
    "\n",
    "# Labels and legend\n",
    "ax.set_xlabel(\"Version\")\n",
    "ax.set_ylabel(\"Number of Unique Repositories\")\n",
    "ax.set_title(\"Request vs Peer Authentication by Version\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(versions)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0412ce6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python script to generate LaTeX table with counts and percentages for unique and starred\n",
    "\n",
    "versions = [\"v1\", \"v1beta1\", \"v1alpha1\"]\n",
    "kinds = [\"request\", \"peer\"]\n",
    "\n",
    "unique_dfs = {\n",
    "    \"v1\": df_v1_unique,\n",
    "    \"v1beta1\": df_v1beta1_unique,\n",
    "    \"v1alpha1\": df_v1alpha1_unique\n",
    "}\n",
    "\n",
    "star_dfs = {\n",
    "    \"v1\": df_v1_star,\n",
    "    \"v1beta1\": df_v1beta1_star,\n",
    "    \"v1alpha1\": df_v1alpha1_star\n",
    "}\n",
    "\n",
    "# Initialize LaTeX table string\n",
    "latex_table = r\"\"\"\\begin{table}[h!]\n",
    "\\centering\n",
    "\\begin{tabular}{lccc}\n",
    "\\hline\n",
    " & v1 & v1beta1 & v1alpha1 \\\\\n",
    "\\hline\n",
    "\"\"\"\n",
    "\n",
    "for kind in kinds:\n",
    "    row = kind.replace(\"_\", \" \").title() + \" Authentication\"\n",
    "    for version in versions:\n",
    "        # Counts\n",
    "        df_unique = unique_dfs[version]\n",
    "        df_star = star_dfs[version]\n",
    "\n",
    "        unique_count = len(df_unique[df_unique[\"kind\"] == kind])\n",
    "        star_count = len(df_star[df_star[\"kind\"] == kind])\n",
    "\n",
    "        # Percentages relative to total unique/starred for that version\n",
    "        total_unique = len(df_unique)\n",
    "        total_star = len(df_star)\n",
    "\n",
    "        pct_unique = (unique_count / total_unique * 100) if total_unique > 0 else 0\n",
    "        pct_star = (star_count / total_star * 100) if total_star > 0 else 0\n",
    "\n",
    "        row += f\" & {unique_count} ({pct_unique:.1f}\\\\%) / {star_count} ({pct_star:.1f}\\\\%)\"\n",
    "    row += r\" \\\\\"\n",
    "    latex_table += row + \"\\n\"\n",
    "\n",
    "latex_table += r\"\"\"\\hline\n",
    "\\end{tabular}\n",
    "\\caption{Number of unique and starred repositories (≥10 stars) using request vs peer authentication across different API versions. Format: unique (percentage) / starred (percentage).}\n",
    "\\label{tab:req_peer_versions_star_pct}\n",
    "\\end{table}\"\"\"\n",
    "\n",
    "print(latex_table)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
